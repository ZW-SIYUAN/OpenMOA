================================================================================
FTRL-Proximal Experimental Results
McMahan et al. (2011) Replication
================================================================================

Implementation:
  - Sparse FTRL with lazy weight computation
  - BagOfWordsStream with unit-length normalization
  - λ = 0.05 (direct value, not 0.05/T)
  - 5 random shuffles averaged

Parameters:
  λ (L1):      0.05
  β (beta):    1.0
  L2:          1.0
  Shuffles:    5
  Instances:   2000 per dataset

================================================================================
RESULTS (Paper Table 2 Format)
================================================================================

Dataset         Your Result               Paper Result              Diff
--------------------------------------------------------------------------------
books           71.6 (0.105)              87.4 (0.081)              -15.8%
dvd             73.5 (0.105)              88.4 (0.078)              -14.9%
electronics     77.4 (0.151)              91.6 (0.114)              -14.2%
kitchen         79.2 (0.182)              93.1 (0.129)              -13.9%

================================================================================
DETAILED STATISTICS
================================================================================

Dataset         Accuracy ± Std            Density ± Std             Best α
--------------------------------------------------------------------------------
books           71.560 ± 0.721        0.105 ± 0.001        1.9
dvd             73.530 ± 0.208        0.105 ± 0.001        1.9
electronics     77.410 ± 0.216        0.151 ± 0.001        1.9
kitchen         79.180 ± 0.540        0.182 ± 0.002        1.9

================================================================================
ANALYSIS
================================================================================

Observations:
  1. Sparsity matches paper well (80-90% zero weights)
  2. Accuracy is 10-15% lower than paper
  3. Possible reasons for accuracy gap:
     - Larger vocabulary size (195k vs ~7k words)
     - Different preprocessing pipeline
     - Dataset version differences

Key findings:
  - FTRL-Proximal successfully achieves high sparsity
  - L1 regularization (λ=0.05) effectively prunes features
  - Results are consistent across 5 shuffles (low std)
